{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e84deb34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T20:48:50.238905Z",
     "start_time": "2023-06-23T20:48:50.083751Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import string\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from itertools import compress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13238adc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T20:48:50.269750Z",
     "start_time": "2023-06-23T20:48:50.249078Z"
    }
   },
   "outputs": [],
   "source": [
    "#settings (do not change it)\n",
    "\n",
    "n_context = 5\n",
    "trainx_prob = \"75\"\n",
    "all_passage_set = set([1,2,3,4,5])\n",
    "\n",
    "instruction_no_pert_aware = \\\n",
    "\"\"\"\n",
    "Refer to the above passages and your knowledge, and answer the following question. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "instruction_pert_aware = \\\n",
    "\"\"\"\n",
    "Refer to the above passages and your knowledge, and answer the following question. Some passages may have been perturbed with wrong information. If there are passages that have been perturbed, find the perturbed passages and ignore them when eliciting the correct answer. If there is no perturbed passage, skip the process of finding the perturbed passage and derive the answer directly.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def pert_inst_generator(pert_lbls):\n",
    "    pas_label = set()\n",
    "    pert_text = \"perturbed: \"\n",
    "    for pid, pas in enumerate(pert_lbls):\n",
    "        if pas == 1:\n",
    "            pas_label.add(pid+1)\n",
    "    if len(pas_label) == 0:\n",
    "        pert_text += \"No passages are perturbed. \"\n",
    "    else:\n",
    "        pert_text += \"Passage \" + \", \".join([str(l) for l in pas_label]) + (\" is\" if len(pas_label) == 1 else \" are\") + \" perturbed. \"\n",
    "    remaining_passage = all_passage_set - pas_label\n",
    "    if len(remaining_passage) == 0:\n",
    "        pert_text += \"Deriving the answer with ignoring all the passages.\"\n",
    "    else:\n",
    "        pert_text += \"Deriving the answer based on Passage \" + \", \".join([str(l) for l in remaining_passage]) + \".\"\n",
    "    return pert_text\n",
    "\n",
    "def prompt_from_nqopen(nq_data_ins, pert_lbls_str, is_pert_prompt=False, pert_fid=None, post_fix=None, inst_at_end = None):\n",
    "    oneshot_prompt_example = \"\"\n",
    "    prefix_original = \"title: \"\n",
    "    prefix_new = \"Passage \"\n",
    "    pert_lbls = nq_data_ins['pert_lbls' + '_' + pert_lbls_str]\n",
    "    \n",
    "    for i, passage in enumerate(nq_data_ins['ctxs']):\n",
    "        passage_to_add = prefix_new + str(i+1) + \": \"\n",
    "        if pert_lbls[i] == 0:\n",
    "            passage_to_add += passage['text'][len(prefix_original):]\n",
    "        else:\n",
    "            passage_to_add += passage['text_pert']\n",
    "        oneshot_prompt_example += passage_to_add.strip()\n",
    "        oneshot_prompt_example += \"\\n\"\n",
    "        \n",
    "    if not inst_at_end is None:\n",
    "        oneshot_prompt_example += inst_at_end\n",
    "    oneshot_prompt_example += \"question: \" + nq_data_ins['question'].strip() + \"?\\n\"\n",
    "    if post_fix is None: #sample\n",
    "        if is_pert_prompt:\n",
    "            oneshot_prompt_example += pert_inst_generator(pert_lbls) + \"\\n\"\n",
    "        oneshot_prompt_example += \"answer: \" + nq_data_ins['answers'][0] + \"\\n\\n\"\n",
    "    else:\n",
    "        if not pert_fid is None:\n",
    "            oneshot_prompt_example += pert_inst_generator(pert_fid) + \"\\n\"\n",
    "        oneshot_prompt_example += post_fix\n",
    "    return oneshot_prompt_example\n",
    "\n",
    "def api_GPT_oneshot_pinst(prompt, api_key=None, model=\"text-davinci-003\"):\n",
    "    headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': 'Bearer ' + api_key\n",
    "    }\n",
    "\n",
    "    json_data = {\n",
    "        'model': model,\n",
    "        'prompt': prompt,\n",
    "        'max_tokens': 256,\n",
    "        'temperature': 0.0,\n",
    "        'top_p':1,\n",
    "        'frequency_penalty':0.0,\n",
    "        #'best_of':1,\n",
    "        'presence_penalty':0,\n",
    "        'logprobs':10,\n",
    "    }\n",
    "    \n",
    "    response = requests.post('https://api.openai.com/v1/completions', headers=headers, json=json_data)\n",
    "    return response\n",
    "\n",
    "def normalize_answer(s):\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return normalize_answer(prediction) == normalize_answer(ground_truth)\n",
    "\n",
    "def ems(prediction, ground_truths):\n",
    "    return max([exact_match_score(prediction, gt) for gt in ground_truths])\n",
    "\n",
    "def em_calc(predictions, n_sample = -1):\n",
    "    predictions_refined = []\n",
    "    for predidx, pred in enumerate(predictions):\n",
    "        pred = pred['choices'][0]['text'].strip()\n",
    "        if \"answer:\" in pred:\n",
    "            predictions_refined.append(pred[pred.find(\"answer:\")+8:])\n",
    "        elif \"Answer:\" in pred:\n",
    "            predictions_refined.append(pred[pred.find(\"Answer:\")+8:])\n",
    "        else:\n",
    "            predictions_refined.append(pred)\n",
    "    em_score = np.mean([ems(pred, sample_answers[i]) for i, pred in enumerate(predictions_refined[:n_sample])])\n",
    "    return em_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aee7bc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T20:48:50.279707Z",
     "start_time": "2023-06-23T20:48:50.276366Z"
    }
   },
   "outputs": [],
   "source": [
    "#config\n",
    "\n",
    "GPT3_api_key = \"\" #your api key id for openai\n",
    "is_dev = False #True: dev-256 dataset, False: test dataset\n",
    "\n",
    "use_parametric_only = False #if True, use only parametric setting and ignore below two settings\n",
    "use_pert_aware_instruction = True #True: instructions are perturbation-aware, False: instruction are not perturbation-aware\n",
    "use_discriminator_fid = True #True: inject fid discriminator's prediction results in prompts, False: let GPT-3 to generate perturbation predictions\n",
    "\n",
    "pert_ratio = '35' #perturbation probability ['00', '15', '25', '35']\n",
    "train_sample_idx = 0 #which set of samples to use? should be one of [0,1,2,3,4]\n",
    "\n",
    "train_sample_path = \"../../DATA/GPT/GPT_trainx_longpre_samples.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5011efa0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T20:48:50.511416Z",
     "start_time": "2023-06-23T20:48:50.428791Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(train_sample_path, 'r') as f:\n",
    "    trainx_ins_list = json.load(f)\n",
    "\n",
    "if is_dev:\n",
    "    dataset_path = \"../../DATA/corpus/NQ_eval_longpre_dev_256_new_fix.json\"\n",
    "    discriminator_fid_path = \"../../DATA/GPT/Discriminator_FiD_predictions_contra_nq_dev_longpre_256.pkl\"\n",
    "else:\n",
    "    dataset_path = \"../../DATA/corpus/NQ_eval_longpre_test_fix\"\n",
    "    discriminator_fid_path = \"../../DATA/GPT/Discriminator_FiD_predictions_contra_nq_test_longpre_full.pkl\"\n",
    "\n",
    "with open(dataset_path, 'r') as f:\n",
    "    dataset = json.load(f)\n",
    "with open(discriminator_fid_path, 'rb') as f:\n",
    "    discriminator_fid_pred = pickle.load(f)\n",
    "\n",
    "sample_n = len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bfeff77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-24T02:18:49.359308Z",
     "start_time": "2023-06-24T02:17:07.861848Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [01:41<00:00,  3.27s/it]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    predictions\n",
    "except NameError:\n",
    "    predictions = []\n",
    "\n",
    "start_from = len(predictions)\n",
    "\n",
    "if use_parametric_only:\n",
    "    prompt = \"question: \" + trainx_ins_list[train_sample_idx]['question'].strip() + \"?\\n\" + \"answer: \"+ trainx_ins_list[train_sample_idx]['answers'][0] + \"\\n\\n\"\n",
    "else:\n",
    "    post_fix = \"answer:\"\n",
    "    if use_pert_aware_instruction:\n",
    "        instruction = instruction_pert_aware\n",
    "        if use_discriminator_fid == False:\n",
    "            post_fix = \"perturbed:\" # GPT-3 should generate predictions for perturbation (discriminator_inst setting)\n",
    "    else:\n",
    "        instruction = instruction_no_pert_aware\n",
    "    prompt = prompt_from_nqopen(trainx_ins_list[train_sample_idx], pert_lbls_str=trainx_prob, is_pert_prompt=use_pert_aware_instruction, inst_at_end = instruction)\n",
    "    \n",
    "\n",
    "for idx, ins in enumerate(tqdm(dataset[start_from:sample_n])):\n",
    "    if use_parametric_only:\n",
    "        prompt_test = \"question: \" + ins['question'].strip() + \"?\\n\" + \"answer: \"\n",
    "    else:\n",
    "        pert_fid_label = discriminator_fid_pred[pert_ratio][start_from + idx] if use_discriminator_fid else None #fid clssification results\n",
    "        prompt_test = prompt_from_nqopen(ins, pert_lbls_str=pert_ratio, is_pert_prompt=use_pert_aware_instruction, pert_fid=pert_fid_label, post_fix = post_fix)\n",
    "    result = api_GPT_oneshot_pinst(prompt +  prompt_test, api_key = GPT3_api_key, model = 'text-davinci-003')\n",
    "    assert 'choices' in result.json()\n",
    "    predictions.append(result.json())\n",
    "assert len(predictions) == sample_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7f3b67f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-24T02:22:22.271839Z",
     "start_time": "2023-06-24T02:22:22.205830Z"
    }
   },
   "outputs": [],
   "source": [
    "#store result\n",
    "list_to_store = [prompt]\n",
    "list_to_store += predictions\n",
    "\n",
    "gpt_outpath = \"nq_longpre_GPT_outputs\"\n",
    "filename = \"dev_\" if is_dev else \"test_\"\n",
    "filename += \"para_\" if use_parametric_only else \"semipara_\"\n",
    "\n",
    "if use_parametric_only == False:\n",
    "    filename += \"pert_\" if use_pert_aware_instruction else \"\"\n",
    "    filename += \"fidpred_\" if use_discriminator_fid else \"\"\n",
    "    filename += \"p\" + pert_ratio + \"_\"\n",
    "filename +=  \"sample\" + str(train_sample_idx) + \".pkl\"\n",
    "\n",
    "with open(os.path.join(gpt_outpath, filename), 'wb') as output:\n",
    "    pickle.dump(list_to_store, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d11fac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ACL2023)",
   "language": "python",
   "name": "acl2023_submission"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
